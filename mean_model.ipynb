{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n",
      "/opt/homebrew/Caskroom/miniconda/base/bin/python\n",
      "   username             movie_name  rating\n",
      "0  mmoorthy        mean-girls-2024     6.0\n",
      "1  mmoorthy         anyone-but-you     4.0\n",
      "2  mmoorthy    society-of-the-snow    10.0\n",
      "3  mmoorthy               saltburn     8.0\n",
      "4  mmoorthy  no-hard-feelings-2023     4.0\n",
      "Data types: username       object\n",
      "movie_name     object\n",
      "rating        float64\n",
      "dtype: object\n",
      "Count of non-NA values:\n",
      " username      19687685\n",
      "movie_name    19687685\n",
      "rating        15150737\n",
      "dtype: int64\n",
      "   username       movie_name  rating\n",
      "0  mmoorthy  mean-girls-2024     6.0\n",
      "1  mmoorthy   anyone-but-you     4.0\n",
      "Index(['movie_name', 'real_movie_name', 'director', 'actors', 'genres'], dtype='object')\n",
      "hi2\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "print(tf.__version__)\n",
    "print(sys.executable)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow_recommenders as tfrs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Connect to SQLite database and load movie details\n",
    "conn = sqlite3.connect('my_letterboxd_data.db')\n",
    "\n",
    "# Load ratings data\n",
    "query_ratings = \"\"\"\n",
    "SELECT username, movie_name, rating\n",
    "FROM users\n",
    "\"\"\"\n",
    "ratings_df = pd.read_sql(query_ratings, conn)\n",
    "# Check if the DataFrame is empty or if specific columns are empty\n",
    "print(ratings_df.head())\n",
    "print(\"Data types:\", ratings_df.dtypes)\n",
    "print(\"Count of non-NA values:\\n\", ratings_df.count())\n",
    "# Load movie details\n",
    "query_movie_details = \"\"\"\n",
    "SELECT letterboxd_slug, movie_name, director, actors, genres\n",
    "FROM film_details_small\n",
    "\"\"\"\n",
    "movies_details_df = pd.read_sql(query_movie_details, conn) # REAL MOVIE NAME\n",
    "# rename columns from movie_details_df\n",
    "movies_details_df.rename(columns={'movie_name': 'real_movie_name'}, inplace=True)\n",
    "movies_details_df.rename(columns={'letterboxd_slug': 'movie_name'}, inplace=True)\n",
    "conn.close()\n",
    "\n",
    "\n",
    "# get list of unique movie names sorted by count of ratings from ratings_df\n",
    "movie_names = ratings_df['movie_name'].value_counts().index.tolist()\n",
    "\n",
    "# dump the top 5000 movie names to a file\n",
    "with open('movie_names.txt', 'w') as f:\n",
    "    for item in movie_names[:5000]:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "# Data preprocessing\n",
    "ratings_df['rating'] = ratings_df['rating'].astype(float)\n",
    "ratings_df = ratings_df.fillna(-1)\n",
    "movies_details_df.fillna('', inplace=True)  # Handle missing values\n",
    "print(ratings_df.head(2))\n",
    "# Merge ratings with movie details\n",
    "df = pd.merge(ratings_df, movies_details_df, on='movie_name', how='left')\n",
    "\n",
    "# get list of unique movie names\n",
    "movie_names = movies_details_df['movie_name'].unique()\n",
    "# # Encoding categorical features\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# import joblib  # Import joblib\n",
    "\n",
    "# def encode_and_save_column(column, name):\n",
    "#     encoder = LabelEncoder()\n",
    "#     transformed = encoder.fit_transform(column)\n",
    "#     joblib.dump(encoder, f'{name}_encoder.joblib')  # Save the encoder\n",
    "#     return transformed, len(encoder.classes_)\n",
    "\n",
    "# for feature in ['username', 'movie_name', 'director', 'actors', 'genres']:\n",
    "#     df[feature], num_classes = encode_and_save_column(df[feature],feature)\n",
    "#     df[feature] = df[feature].astype('int64')\n",
    "\n",
    "# print('hi')\n",
    "# # Convert to TensorFlow dataset\n",
    "# def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "#     df = dataframe.copy()\n",
    "#     labels = df.pop('rating')\n",
    "#     ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "#     if shuffle:\n",
    "#         ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "#     ds = ds.batch(batch_size)\n",
    "#     return ds\n",
    "print(movies_details_df.columns)\n",
    "\n",
    "print('hi2')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ell\n",
      "19687685\n"
     ]
    }
   ],
   "source": [
    "# create a function that returns the average rating of a movie\n",
    "# filter df to only have movies that appear at least 10 times\n",
    "\n",
    "\n",
    "print('ell')\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "def create_map_movie_to_average_rating():\n",
    "    # Group by 'movie_name' and calculate the mean of 'rating' for each group\n",
    "    movie_to_rating = df.groupby('movie_name')['rating'].mean().to_dict()\n",
    "    return movie_to_rating\n",
    "\n",
    "m_to_r = create_map_movie_to_average_rating()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316184/316184 [00:00<00:00, 998064.95it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('007', 10.0), ('08ms', 10.0), ('1-100-rice-planting', 10.0), ('1-3', 10.0), ('1-800-d-direct', 10.0), ('1-ri-botchi-no-ookami-to-7-hiki-no-ko-yagi', 10.0), ('10-45-in-a-city-like-any-other', 10.0), ('100-jahre-kino', 10.0), ('100-percent-electrical', 10.0), ('100-renewable-energy', 10.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create a function that returns the top n movies for a user that they have not rated yet using get_movie_recommendations \n",
    "def get_user_recommendations(username, n_recommendations=10):\n",
    "    # get all the movies that the user has not rated\n",
    "    user_rated_movies = df[df['username'] == username]['movie_name']\n",
    "    all_movies = df['movie_name'].unique()\n",
    "    movies_to_rate = np.setdiff1d(all_movies, user_rated_movies)\n",
    "    recommendations = []\n",
    "    for movie in tqdm(movies_to_rate):\n",
    "        recommendations.append((movie, m_to_r[movie]))\n",
    "    recommendations = sorted(recommendations, key=lambda x: x[1], reverse=True)\n",
    "    return recommendations[0:n_recommendations]\n",
    "\n",
    "# get the top 10 movie recommendations for a user\n",
    "print(get_user_recommendations('nconterno', 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.323628331235552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:02<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8947485126034658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_mse(df, m_to_r, n_users=10):\n",
    "    mse = 0\n",
    "    count = 0\n",
    "    users = df['username'].unique()[:n_users]\n",
    "    for user in tqdm(users):\n",
    "        user_data = df[df['username'] == user]\n",
    "        user_rated_movies = user_data['movie_name']\n",
    "        for index, row in user_data.iterrows():\n",
    "            predicted_rating = m_to_r[row['movie_name']]\n",
    "            actual_rating = row['rating']\n",
    "            mse += (predicted_rating - actual_rating) ** 2\n",
    "            count += 1\n",
    "    return mse / count if count else 0\n",
    "\n",
    "# Example usage\n",
    "print(compute_mse(df, m_to_r, 100))\n",
    "\n",
    "def compute_mae(df, m_to_r, n_users=10):\n",
    "    mae = 0\n",
    "    count = 0\n",
    "    users = df['username'].unique()[:n_users]\n",
    "    for user in tqdm(users):\n",
    "        user_data = df[df['username'] == user]\n",
    "        user_rated_movies = user_data['movie_name']\n",
    "        for index, row in user_data.iterrows():\n",
    "            predicted_rating = m_to_r[row['movie_name']]\n",
    "            actual_rating = row['rating']\n",
    "            mae += abs(predicted_rating - actual_rating)\n",
    "            count += 1\n",
    "    return mae / count if count else 0\n",
    "\n",
    "# Example usage\n",
    "print(compute_mae(df, m_to_r, 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby('movie_name').filter(lambda x: len(x) >= 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m data \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39mload_from_df(df[[\u001b[39m'\u001b[39m\u001b[39musername\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmovie_name\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrating\u001b[39m\u001b[39m'\u001b[39m]], reader)\n\u001b[1;32m     11\u001b[0m algo \u001b[39m=\u001b[39m SVD()\n\u001b[0;32m---> 12\u001b[0m cross_validate(algo, data, measures\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mRMSE\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mMAE\u001b[39;49m\u001b[39m'\u001b[39;49m], cv\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/surprise/model_selection/validation.py:108\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(algo, data, measures, cv, return_train_measures, n_jobs, pre_dispatch, verbose)\u001b[0m\n\u001b[1;32m    102\u001b[0m cv \u001b[39m=\u001b[39m get_cv(cv)\n\u001b[1;32m    104\u001b[0m delayed_list \u001b[39m=\u001b[39m (\n\u001b[1;32m    105\u001b[0m     delayed(fit_and_score)(algo, trainset, testset, measures, return_train_measures)\n\u001b[1;32m    106\u001b[0m     \u001b[39mfor\u001b[39;00m (trainset, testset) \u001b[39min\u001b[39;00m cv\u001b[39m.\u001b[39msplit(data)\n\u001b[1;32m    107\u001b[0m )\n\u001b[0;32m--> 108\u001b[0m out \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49mn_jobs, pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch)(delayed_list)\n\u001b[1;32m    110\u001b[0m (test_measures_dicts, train_measures_dicts, fit_times, test_times) \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mout)\n\u001b[1;32m    112\u001b[0m test_measures \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/joblib/parallel.py:1844\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1841\u001b[0m \u001b[39myield\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1843\u001b[0m \u001b[39m# Sequentially call the tasks and yield the results.\u001b[39;00m\n\u001b[0;32m-> 1844\u001b[0m \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1845\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/surprise/model_selection/validation.py:104\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    100\u001b[0m measures \u001b[39m=\u001b[39m [m\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m measures]\n\u001b[1;32m    102\u001b[0m cv \u001b[39m=\u001b[39m get_cv(cv)\n\u001b[0;32m--> 104\u001b[0m delayed_list \u001b[39m=\u001b[39m (\n\u001b[1;32m    105\u001b[0m     delayed(fit_and_score)(algo, trainset, testset, measures, return_train_measures)\n\u001b[1;32m    106\u001b[0m     \u001b[39mfor\u001b[39;00m (trainset, testset) \u001b[39min\u001b[39;00m cv\u001b[39m.\u001b[39msplit(data)\n\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    108\u001b[0m out \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)(delayed_list)\n\u001b[1;32m    110\u001b[0m (test_measures_dicts, train_measures_dicts, fit_times, test_times) \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mout)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/surprise/model_selection/split.py:117\u001b[0m, in \u001b[0;36mKFold.split\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    112\u001b[0m raw_trainset \u001b[39m=\u001b[39m [\n\u001b[1;32m    113\u001b[0m     data\u001b[39m.\u001b[39mraw_ratings[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m chain(indices[:start], indices[stop:])\n\u001b[1;32m    114\u001b[0m ]\n\u001b[1;32m    115\u001b[0m raw_testset \u001b[39m=\u001b[39m [data\u001b[39m.\u001b[39mraw_ratings[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m indices[start:stop]]\n\u001b[0;32m--> 117\u001b[0m trainset \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mconstruct_trainset(raw_trainset)\n\u001b[1;32m    118\u001b[0m testset \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mconstruct_testset(raw_testset)\n\u001b[1;32m    120\u001b[0m \u001b[39myield\u001b[39;00m trainset, testset\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/surprise/dataset.py:206\u001b[0m, in \u001b[0;36mDataset.construct_trainset\u001b[0;34m(self, raw_trainset)\u001b[0m\n\u001b[1;32m    203\u001b[0m         raw2inner_id_items[irid] \u001b[39m=\u001b[39m current_i_index\n\u001b[1;32m    204\u001b[0m         current_i_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 206\u001b[0m     ur[uid]\u001b[39m.\u001b[39;49mappend((iid, r))\n\u001b[1;32m    207\u001b[0m     ir[iid]\u001b[39m.\u001b[39mappend((uid, r))\n\u001b[1;32m    209\u001b[0m n_users \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(ur)  \u001b[39m# number of users\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# implement svd model and compute mse and mae\n",
    "from surprise import Dataset\n",
    "\n",
    "from surprise import Reader\n",
    "from surprise import SVD\n",
    "\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "reader = Reader(rating_scale=(0, 10))\n",
    "data = Dataset.load_from_df(df[['username', 'movie_name', 'rating']], reader)\n",
    "algo = SVD()\n",
    "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting surprise\n",
      "  Using cached surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
      "Collecting scikit-surprise\n",
      "  Using cached scikit-surprise-1.1.3.tar.gz (771 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.0.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from scikit-surprise->surprise) (1.4.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from scikit-surprise->surprise) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from scikit-surprise->surprise) (1.13.0)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp310-cp310-macosx_11_0_arm64.whl size=1114515 sha256=090e7aab243e78b8d63e45bebb1714fa46bd0505184933cf159192140cec298c\n",
      "  Stored in directory: /Users/nicholasconterno/Library/Caches/pip/wheels/df/e4/a6/7ad72453dd693f420b0c639bedeec34641738d11b55d8d9b84\n",
      "Successfully built scikit-surprise\n",
      "Installing collected packages: scikit-surprise, surprise\n",
      "Successfully installed scikit-surprise-1.1.3 surprise-0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install surprise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
